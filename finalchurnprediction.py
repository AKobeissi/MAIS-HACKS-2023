# -*- coding: utf-8 -*-
"""Finalchurnprediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QJigbdqUIvIoEfIuXcnssdA96H4HvhDi
"""

#import packages
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.decomposition import PCA
from sklearn.tree import DecisionTreeClassifier, export_graphviz
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from lightgbm import LGBMClassifier
from sklearn.metrics import roc_auc_score, recall_score, confusion_matrix, classification_report
import subprocess
import joblib

#get multiple output in one cell
from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = "all"

#import dataset
churn = pd.read_csv("Churn_Modelling.csv")

"""## Data exploration"""

#overview of the data
print("Dataset Overview:")
print(churn.head())

#shape
print("\nDataset Shape:")
print(churn.shape)

#check for Nan
print("\nMissing Values:")
print(churn.isnull().sum())

#check data types
print("\nData Types:")
print(churn.dtypes)

#summary statistics
print("\nSummary Statistics:")
print(churn.describe())

#categorical columns
categorical_columns = churn.select_dtypes(include=['object']).columns
for col in categorical_columns:
    print(f"\nUnique values in {col}:")
    print(churn[col].value_counts())

# Data visualization

#age
plt.figure(figsize=(8, 6))
plt.hist(churn['Age'], bins=20, color='skyblue', edgecolor='black')
plt.title('Distribution of Age')
plt.xlabel('Age')
plt.ylabel('Count')
plt.show()

#gender
plt.figure(figsize=(8, 6))
churn['Gender'].value_counts().plot(kind='bar', color='salmon')
plt.title('Distribution of Gender')
plt.xlabel('Gender')
plt.ylabel('Count')
plt.show()


#correlation matrix
correlation_matrix = churn.corr()
plt.figure(figsize=(10, 8))
plt.matshow(correlation_matrix, cmap='coolwarm')
plt.colorbar()
plt.title('Correlation Matrix')
plt.xticks(range(len(correlation_matrix.columns)), correlation_matrix.columns, rotation='vertical')
plt.yticks(range(len(correlation_matrix.columns)), correlation_matrix.columns)
plt.show()

"""## Data Preprocessing"""

#churn = pd.read_csv("Churn_Modelling.csv")

remove = ["RowNumber"] #don't bring anything relevant to the analysis
churn.drop(remove, axis=1)

y = churn['Exited'] #target variable

#categorical variable gender
le = LabelEncoder()
churn["Gender"] = le.fit_transform(churn["Gender"])
churn.head()

#test set aside
churn_train_val, churn_test, y_train_val, y_test = train_test_split(churn, y.ravel(), test_size = 0.1, random_state = 42)

#splitting into train and validation set
churn_train, churn_val, y_train, y_val = train_test_split(churn_train_val, y_train_val, test_size = 0.12, random_state = 42)
churn_train.shape, churn_val.shape, churn_test.shape, y_train.shape, y_val.shape, y_test.shape

np.mean(y_train), np.mean(y_val), np.mean(y_test)

churn_train_val_CUSTID = churn_train_val[['CustomerId',"Surname"]]
churn_val_CUSTID = churn_val[['CustomerId',"Surname"]]
churn_test_CUSTID = churn_test[['CustomerId',"Surname"]]

from sklearn.preprocessing import OneHotEncoder, LabelEncoder

le_ohe = LabelEncoder()
ohe = OneHotEncoder(sparse=False)
ohe_train = ohe.fit_transform(le_ohe.fit_transform(churn_train['Geography']).reshape(-1, 1))
le_ohe_geography_mapping = dict(zip(le_ohe.classes_, range(len(le_ohe.classes_))))
le_ohe_geography_mapping

#validation + test set
enc_val = churn_val['Geography'].map(le_ohe_geography_mapping).fillna(9999).astype(int).values.reshape(-1, 1)
enc_test = churn_test['Geography'].map(le_ohe_geography_mapping).fillna(9999).astype(int).values.reshape(-1, 1)

ohe_val = ohe.transform(enc_val)
ohe_test = ohe.transform(enc_test)

#new columns
cols = ['country_' + str(x) for x in le_ohe_geography_mapping.keys()]
cols

#adding to the df
churn_train = pd.concat([churn_train.reset_index(), pd.DataFrame(ohe_train, columns = cols)], axis = 1).drop(['index'], axis=1)
churn_val = pd.concat([churn_val.reset_index(), pd.DataFrame(ohe_val, columns = cols)], axis = 1).drop(['index'], axis=1)
churn_test = pd.concat([churn_test.reset_index(), pd.DataFrame(ohe_test, columns = cols)], axis = 1).drop(['index'], axis=1)
#training set
#churn_train.head()
#validation set
#churn_val.head()
#test set
#churn_test.head()

churn_test

#Drop original columns
churn_train.drop(['Geography'], axis=1, inplace=True)
churn_val.drop(['Geography'], axis=1, inplace=True)
churn_test.drop(['Geography'], axis=1, inplace=True)

#Encoding surname
means = churn_train.groupby(['Surname']).Exited.mean()
means.head()
means.tail()

global_mean = y_train.mean()

#new encoding surname
churn_train['Surname_mean_churn'] = churn_train.Surname.map(means)
churn_train['Surname_mean_churn'].fillna(global_mean, inplace=True)

freqs = churn_train.groupby(['Surname']).size()
freqs.head()

churn_train['Surname_freq'] = churn_train.Surname.map(freqs)
churn_train['Surname_freq'].fillna(0, inplace=True)

churn_train['Surname_enc'] = ((churn_train.Surname_freq * churn_train.Surname_mean_churn) - churn_train.Exited)/(churn_train.Surname_freq - 1)
# Fill nans according to frequency being above or lower than 1
churn_train['Surname_enc'].fillna((((churn_train.shape[0] * global_mean) - churn_train.Exited) / (churn_train.shape[0] - 1)), inplace=True)
#churn_train.head(5)

#Replacing by category means
churn_val["Surname_enc"] = churn_test.Surname.map(means)
churn_val['Surname_enc'].fillna(global_mean, inplace=True)
churn_test['Surname_enc'] = churn_test.Surname.map(means)
churn_test['Surname_enc'].fillna(global_mean, inplace=True)
# Show that using LOO Target encoding decorrelates features
churn_train[['Surname_mean_churn', 'Surname_enc', 'Exited']].corr()

churn_testIDS = churn_test[['CustomerId',"Surname"]]


churn_trainIDS = churn_train[['CustomerId',"Surname"]]
churn_valIDS = churn_val[['CustomerId',"Surname"]]

#drop old columns
churn_train.drop(['Surname_mean_churn'], axis=1, inplace=True)
churn_train.drop(['Surname_freq'], axis=1, inplace=True)
churn_train.drop(['Surname'], axis=1, inplace=True)
churn_val.drop(['Surname'], axis=1, inplace=True)
churn_test.drop(['Surname'], axis=1, inplace=True)
#churn_train.head()

#new correlation table/matrix
corr = churn_train.corr()
corr
sns.heatmap(corr, cmap = 'coolwarm')

#products with correlation
eps = 1e-6

churn_train['bal_per_product'] = churn_train.Balance/(churn_train.NumOfProducts + eps)
churn_train['bal_by_est_salary'] = churn_train.Balance/(churn_train.EstimatedSalary + eps)
churn_train['tenure_age_ratio'] = churn_train.Tenure/(churn_train.Age + eps)
churn_train['age_surname_mean_churn'] = np.sqrt(churn_train.Age) * churn_train.Surname_enc

new_cols = ['bal_per_product', 'bal_by_est_salary', 'tenure_age_ratio', 'age_surname_mean_churn']

churn_train[new_cols].isnull().sum()

#linear association of new columns with "exited" to see weights
sns.heatmap(churn_train[new_cols + ['Exited']].corr(), annot=True)

#notice bal per product and tenure age ratio

churn_val['bal_per_product'] = churn_val.Balance/(churn_val.NumOfProducts + eps)
churn_val['bal_by_est_salary'] = churn_val.Balance/(churn_val.EstimatedSalary + eps)
churn_val['tenure_age_ratio'] = churn_val.Tenure/(churn_val.Age + eps)
churn_val['age_surname_mean_churn'] = np.sqrt(churn_val.Age) * churn_val.Surname_enc
churn_test['bal_per_product'] = churn_test.Balance/(churn_test.NumOfProducts + eps)
churn_test['bal_by_est_salary'] = churn_test.Balance/(churn_test.EstimatedSalary + eps)
churn_test['tenure_age_ratio'] = churn_test.Tenure/(churn_test.Age + eps)
churn_test['age_surname_mean_churn'] = np.sqrt(churn_test.Age) * churn_test.Surname_enc

# initialize the standard scaler
sc = StandardScaler()
cont_vars = ['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'EstimatedSalary', 'Surname_enc', 'bal_per_product'
             , 'bal_by_est_salary', 'tenure_age_ratio', 'age_surname_mean_churn']
cat_vars = ['Gender', 'HasCrCard', 'IsActiveMember', 'country_France', 'country_Germany', 'country_Spain']
#scale continuous columns
cols_to_scale = cont_vars
sc_X_train = sc.fit_transform(churn_train[cols_to_scale])
#array to df
sc_X_train = pd.DataFrame(data=sc_X_train, columns=cols_to_scale)
sc_X_train.shape
sc_X_train.head()

#scaling validation + training set
sc_X_val = sc.transform(churn_val[cols_to_scale])
sc_X_test = sc.transform(churn_test[cols_to_scale])
#array to df
sc_X_val = pd.DataFrame(data=sc_X_val, columns=cols_to_scale)
sc_X_test = pd.DataFrame(data=sc_X_test, columns=cols_to_scale)

#featureset and taregt for RFE model
y = churn_train['Exited'].values
X = churn_train[cat_vars + cont_vars]
X.columns = cat_vars + cont_vars
X.columns

x = LabelEncoder()
churn['Gender'] = x.fit_transform(churn['Gender'])
#churn

# for logistics regression
rfe = RFE(estimator=LogisticRegression(), n_features_to_select=10)
rfe = rfe.fit(X.values, y)

print(rfe.support_)
# The feature ranking, such that ranking_[i] corresponds to the ranking position of the i-th feature
print(rfe.ranking_)

# Logistic regression
mask = rfe.support_.tolist()
selected_feats = [b for a,b in zip(mask, X.columns) if a]
selected_feats

selected_cat_vars = [x for x in selected_feats if x in cat_vars]
selected_cont_vars = [x for x in selected_feats if x in cont_vars]
# Using categorical features and scaled numerical features
X_train = np.concatenate((churn_train[selected_cat_vars].values, sc_X_train[selected_cont_vars].values), axis=1)
X_val = np.concatenate((churn_val[selected_cat_vars].values, sc_X_val[selected_cont_vars].values), axis=1)
X_test = np.concatenate((churn_test[selected_cat_vars].values, sc_X_test[selected_cont_vars].values), axis=1)
# print the shapes
X_train.shape, X_val.shape, X_test.shape

# Obtaining class weights based on the class samples imbalance ratio
_, num_samples = np.unique(y_train, return_counts=True)
weights = np.max(num_samples)/num_samples

weights_dict = dict()
class_labels = [0,1]
# Weights associated with classes
for a,b in zip(class_labels,weights):
    weights_dict[a] = b

weights_dict

#Defining model
lr = LogisticRegression(C=1.0, penalty='l2', class_weight=weights_dict, n_jobs=-1)
#train
lr.fit(X_train, y_train)
print(f'Confusion Matrix: \n{confusion_matrix(y_val, lr.predict(X_val))}')
print(f'Area Under Curve: {roc_auc_score(y_val, lr.predict(X_val))}')
print(f'Recall score: {recall_score(y_val,lr.predict(X_val))}')
print(f'Classification report: \n{classification_report(y_val,lr.predict(X_val))}')



pip install utils

from sklearn.preprocessing import OneHotEncoder
from lightgbm import LGBMClassifier
from sklearn.pipeline import Pipeline

y_train



from sklearn.svm import SVC

from lightgbm import LGBMClassifier
from sklearn.metrics import accuracy_score

LGB = LGBMClassifier()
# Define hyperparameters for the LightGBM model
params = {
    'objective': 'binary',  # For binary classification
    'metric': 'binary_error',  # Metric to optimize (binary classification error)
    'boosting_type': 'gbdt',  # Gradient Boosting Decision Tree
    'num_leaves': 31,  # Maximum number of leaves in one tree
    'learning_rate': 0.05,
    'feature_fraction': 0.9,  # Fraction of features to use in each iteration
}

# Create the LightGBM model
model = LGB.fit(X_train, y)

# Make predictions on the test data
y_pred = model.predict(X_test)

# Convert probabilities to binary predictions (0 or 1)
#y_pred_binary = np.round(y_pred)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
#conf_matrix = confusion_matrix(y_test, y_pred_binary)
#lass_report = classification_report(y_test, y_pred_binary)

# Print the evaluation results
print("Accuracy:", accuracy)
#print("\nConfusion Matrix:\n", conf_matrix)
#print("\nClassification Report:\n", class_report)





from xgboost import XGBClassifier
XGB = XGBClassifier()
model = XGB.fit(X_train, y)

# Make predictions on the test data
y_pred = model.predict(X_test)

# Convert probabilities to binary predictions (0 or 1)
#y_pred_binary = np.round(y_pred)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
#conf_matrix = confusion_matrix(y_test, y_pred_binary)
#lass_report = classification_report(y_test, y_pred_binary)

# Print the evaluation results
print("Accuracy:", accuracy)

import lightgbm as lgb
from sklearn.model_selection import train_test_split, GridSearchCV

param_grid = {
    'num_leaves': [15, 31, 50],           # Maximum number of leaves in one tree
    'learning_rate': [0.05, 0.1, 0.2],   # Learning rate
    'n_estimators': [50, 100, 200],       # Number of boosting rounds
    'max_depth': [3, 5, 7],              # Maximum depth of a tree
    'min_child_samples': [10, 20, 30],   # Minimum number of samples in a leaf
}

# Create a LightGBM classifier
model = lgb.LGBMClassifier(objective='binary', metric='binary_error', boosting_type='gbdt')

# Create a GridSearchCV object to search for the best hyperparameters
grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, n_jobs=-1, scoring='accuracy', verbose=2)

# Fit the GridSearchCV object to the training data
grid_search.fit(X_train, y_train)

# Get the best hyperparameters from the search
best_params = grid_search.best_params_
print("Best Hyperparameters:", best_params)

# Get the best model from the search
best_model = grid_search.best_estimator_

# Make predictions on the test data using the best model
LGBM_y_pred = best_model.predict(X_test)
LGBM_y_pred_Prob = best_model.predict_proba(X_test)
# Evaluate the best model
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)

# Print the evaluation results
print("Accuracy:", accuracy)
print("\nConfusion Matrix:\n", conf_matrix)
print("\nClassification Report:\n", class_report)

import xgboost as xgb

param_grid = {
    'max_depth': [3, 5, 7],
    'learning_rate': [0.05, 0.1, 0.2],
    'n_estimators': [50, 100, 200],
    'min_child_weight': [1, 3, 5],
}

# Create an XGBoost classifier
model = xgb.XGBClassifier(objective='binary:logistic', eval_metric='logloss')

# Create a GridSearchCV object to search for the best hyperparameters
grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, n_jobs=-1, scoring='accuracy', verbose=2)

# Fit the GridSearchCV object to the training data
grid_search.fit(X_train, y_train)

# Get the best hyperparameters from the search
best_params = grid_search.best_params_
print("Best Hyperparameters:", best_params)

# Get the best model from the search
best_model = grid_search.best_estimator_

# Make predictions on the test data using the best model
y_pred = best_model.predict(X_test)

# Evaluate the best model
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)

# Print the evaluation results
print("Accuracy:", accuracy)
print("\nConfusion Matrix:\n", conf_matrix)
print("\nClassification Report:\n", class_report)

import xgboost as xgb
params = {
    'learning_rate': 0.2,
    'max_depth': 3,
    'min_child_weight': 1,
    'n_estimators': 100
}
model = xgb.XGBClassifier(objective='binary:logistic', eval_metric='logloss', params=params)

model.fit(X_train,y_train)

predictionsXGB = model.predict(X_test)

#LGBMPreds = pd.DataFrame(data=LGBM_y_pred, columns=['Prediction'])
LGBMPreds = pd.Series(LGBM_y_pred, name="Predictions")
LGBM_y_pred_Prob = pd.Series
results = pd.concat([churn_test_CUSTID,LGBMPreds.reindex(churn_test_CUSTID.index)], axis=1)

# Make predictions on the test data using the best model
LGBM_y_pred = best_model.predict(X_test)
LGBM_y_pred_Prob = best_model.predict_proba(X_test)[:, 1]  # Probability of the positive class (class 1)

# Create new columns in the 'churn' dataframe for predictions and prediction probabilities
churn_test_CUSTID['Predictions'] = LGBM_y_pred
churn_test_CUSTID['Predictions_Prob'] = LGBM_y_pred_Prob

# Display the updated 'churn' dataframe with predictions

churn_test_CUSTID